
@article{oliva_bayesian_2015,
	title = {Bayesian {Nonparametric} {Kernel}-{Learning}},
	url = {http://arxiv.org/abs/1506.08776},
	abstract = {Kernel methods are ubiquitous tools in machine learning. They have proven to be effective in many domains and tasks. Yet, kernel methods often require the user to select a predefined kernel to build an estimator with. However, there is often little reason for the a priori selection of a kernel. Even if a universal approximating kernel is selected, the quality of the finite sample estimator may be greatly effected by the choice of kernel. Furthermore, when directly applying kernel methods, one typically needs to compute a \$N {\textbackslash}times N\$ Gram matrix of pairwise kernel evaluations to work with a dataset of \$N\$ instances. The computation of this Gram matrix precludes the direct application of kernel methods on large datasets. In this paper we introduce Bayesian nonparmetric kernel (BaNK) learning, a generic, data-driven framework for scalable learning of kernels. We show that this framework can be used for performing both regression and classification tasks and scale to large datasets. Furthermore, we show that BaNK outperforms several other scalable approaches for kernel learning on a variety of real world datasets.},
	urldate = {2016-04-11},
	journal = {arXiv:1506.08776 [stat]},
	author = {Oliva, Junier and Dubey, Avinava and Poczos, Barnabas and Schneider, Jeff and Xing, Eric P.},
	month = jun,
	year = {2015},
	note = {arXiv: 1506.08776},
	keywords = {Statistics - Machine Learning},
	file = {arXiv\:1506.08776 PDF:/Users/Jeremiah/Library/Application Support/Zotero/Profiles/9brk52ti.default/zotero/storage/QSAV4XMM/Oliva et al. - 2015 - Bayesian Nonparametric Kernel-Learning.pdf:application/pdf;arXiv.org Snapshot:/Users/Jeremiah/Library/Application Support/Zotero/Profiles/9brk52ti.default/zotero/storage/SHBGZJNN/Oliva et al. - 2015 - Bayesian Nonparametric Kernel-Learning.html:text/html}
}

@phdthesis{fox_bayesian_2009,
	type = {Thesis},
	title = {Bayesian nonparametric learning of complex dynamical phenomena},
	copyright = {http://dspace.mit.edu/handle/1721.1/7582},
	url = {http://dspace.mit.edu/handle/1721.1/55111},
	abstract = {The complexity of many dynamical phenomena precludes the use of linear models for which exact analytic techniques are available. However, inference on standard nonlinear models quickly becomes intractable. In some cases, Markov switching processes, with switches between a set of simpler models, are employed to describe the observed dynamics. Such models typically rely on pre-specifying the number of Markov modes. In this thesis, we instead take a Bayesian nonparametric approach in defining a prior on the model parameters that allows for flexibility in the complexity of the learned model and for development of efficient inference algorithms. We start by considering dynamical phenomena that can be well-modeled as a hidden discrete Markov process, but in which there is uncertainty about the cardinality of the state space. The standard finite state hidden Markov model (HMM) has been widely applied in speech recognition, digital communications, and bioinformatics, amongst other fields. Through the use of the hierarchical Dirichlet process (HDP), one can examine an HMM with an unbounded number of possible states. We revisit this HDPHMM and develop a generalization of the model, the sticky HDP-HMM, that allows more robust learning of smoothly varying state dynamics through a learned bias towards self-transitions. We show that this sticky HDP-HMM not only better segments data according to the underlying state sequence, but also improves the predictive performance of the learned model. Additionally, the sticky HDP-HMM enables learning more complex, multimodal emission distributions.},
	language = {eng},
	urldate = {2016-03-23},
	school = {Massachusetts Institute of Technology},
	author = {Fox, Emily Beth},
	year = {2009},
	file = {Full Text PDF:/Users/Jeremiah/Library/Application Support/Zotero/Profiles/9brk52ti.default/zotero/storage/4DWAM8MG/Fox - 2009 - Bayesian nonparametric learning of complex dynamic.pdf:application/pdf;Snapshot:/Users/Jeremiah/Library/Application Support/Zotero/Profiles/9brk52ti.default/zotero/storage/XH5BDJXB/55111.html:text/html}
}

@article{ishwaran_gibbs_2001,
	title = {Gibbs {Sampling} {Methods} for {Stick}-{Breaking} {Priors}},
	volume = {96},
	issn = {0162-1459},
	url = {http://dx.doi.org/10.1198/016214501750332758},
	doi = {10.1198/016214501750332758},
	abstract = {A rich and flexible class of random probability measures, which we call stick-breaking priors, can be constructed using a sequence of independent beta random variables. Examples of random measures that have this characterization include the Dirichlet process, its two-parameter extension, the two-parameter Poisson–Dirichlet process, finite dimensional Dirichlet priors, and beta two-parameter processes. The rich nature of stick-breaking priors offers Bayesians a useful class of priors for nonparametric problems, while the similar construction used in each prior can be exploited to develop a general computational procedure for fitting them. In this article we present two general types of Gibbs samplers that can be used to fit posteriors of Bayesian hierarchical models based on stick-breaking priors. The first type of Gibbs sampler, referred to as a Pólya urn Gibbs sampler, is a generalized version of a widely used Gibbs sampling method currently employed for Dirichlet process computing. This method applies to stick-breaking priors with a known Pólya urn characterization, that is, priors with an explicit and simple prediction rule. Our second method, the blocked Gibbs sampler, is based on an entirely different approach that works by directly sampling values from the posterior of the random measure. The blocked Gibbs sampler can be viewed as a more general approach because it works without requiring an explicit prediction rule. We find that the blocked Gibbs avoids some of the limitations seen with the Pólya urn approach and should be simpler for nonexperts to use.},
	number = {453},
	urldate = {2016-04-15},
	journal = {Journal of the American Statistical Association},
	author = {Ishwaran, Hemant and James, Lancelot F.},
	month = mar,
	year = {2001},
	pages = {161--173},
	file = {Snapshot:/Users/Jeremiah/Library/Application Support/Zotero/Profiles/9brk52ti.default/zotero/storage/KGEDE9Z8/Ishwaran and James - 2001 - Gibbs Sampling Methods for Stick-Breaking Priors.html:text/html}
}

@article{teh_hierarchical_2006,
	title = {Hierarchical {Dirichlet} {Processes}},
	volume = {101},
	issn = {0162-1459},
	url = {http://www.jstor.org/stable/27639773},
	abstract = {We consider problems involving groups of data where each observation within a group is a draw from a mixture model and where it is desirable to share mixture components between groups. We assume that the number of mixture components is unknown a priori and is to be inferred from the data. In this setting it is natural to consider sets of Dirichlet processes, one for each group, where the well-known clustering property of the Dirichlet process provides a nonparametric prior for the number of mixture components within each group. Given our desire to tie the mixture models in the various groups, we consider a hierarchical model, specifically one in which the base measure for the child Dirichlet processes is itself distributed according to a Dirichlet process. Such a base measure being discrete, the child Dirichlet processes necessarily share atoms. Thus, as desired, the mixture models in the different groups necessarily share mixture components. We discuss representations of hierarchical Dirichlet processes in terms of a stick-breaking process, and a generalization of the Chinese restaurant process that we refer to as the "Chinese restaurant franchise." We present Markov chain Monte Carlo algorithms for posterior inference in hierarchical Dirichlet process mixtures and describe applications to problems in information retrieval and text modeling.},
	number = {476},
	urldate = {2016-03-23},
	journal = {Journal of the American Statistical Association},
	author = {Teh, Yee Whye and Jordan, Michael I. and Beal, Matthew J. and Blei, David M.},
	year = {2006},
	pages = {1566--1581}
}