---
title: "Hidden Markov Model"
author: "Will Townes"
date: "April 11, 2016"
output: html_document
---

```{r}
library(expm)
library(ggplot2)
source("util.R") #all HMM functions defined here
```

Let $z_t$ denote the hidden mode sequence. Each $z_t$ is a categorical random variable pointing to one of the L possible modes. Conditional on $z_t$, we observe the continuous variable $x_t$. The first model we examine sets L=2 with the hidden states "Big" ("B") and "Small" ("S"). We set the transition probabilities as follows:
$$P(z_t="B"|z_{t-1}="B") = .99$$
$$P(z_t="S"|z_{t-1}="B") = .01$$
$$P(z_t="B"|z_{t-1}="S") = .05$$
$$P(z_t="S"|z_{t-1}="S") = .95$$
The emission probabilities are set as follows:
$$P(x_t|z_t="B")=\mathcal{N}(5,2)$$
$$P(x_t|z_t="S")=\mathcal{N}(-5,2)$$

The first step is to create some simulated data from this HMM.

```{r}
# transition matrix, rows sum to one
transition<-matrix(c(.99,.05,.01,.95),nrow=2) #premultiply by indicator to get new vals
stationary_dist<-eigen(t(transition))$vectors[,1]
stationary_dist/sum(stationary_dist)
transition%^%100
#reveals stationary distribution is c(.83,.17)
categs<-c("B","S")
rownames(transition)<-colnames(transition)<-categs
L<-nrow(transition) #2
Tmax<-1000
z<-rep(NA,Tmax)
z[1]<-"S"
for(t in 2:Tmax){
  z[t]<-sample(categs,1,prob=transition[z[t-1],])
}
table(z)/Tmax #should resemble stationary distribution
mu_vals_map<-list(B=5,S=-5)
x<-sapply(z,function(q){rnorm(1,mean=mu_vals_map[[q]],sd=sqrt(2))})
plotdat<-data.frame(index=1:Tmax,x=x,z=z)
ggplot(data=plotdat)+geom_point(aes(x=index,y=x,colour=z))
```

The simulated data shows the correct pattern matching with the stationary distribution desired. We will now use the forward-backward algorithm to infer the hidden states and see if it matches with the true hidden states.

```{r}
#compute all emission log-probabilities
elps<-vapply(categs,function(q){dnorm(x,mean=mu_vals_map[[q]],sd=sqrt(2),log=TRUE)},FUN.VALUE=rep(0.0,Tmax))
# prev. line is a Tmax by L matrix
# each row is one time step.
# values are on the log-scale
msg_gam<-forward_backward(transition,elps)
#find predicted states most likely, doesn't require normalization!
zpreds<-max.col(msg_gam)
zpreds<-hidden_int2char(zpreds,categs)
table(zpreds,z)
```
We see in the above table that everything was correctly classified by the Forward-Backward Algorithm!

Next we try to use Algorithm 10 "blocked Gibbs" from Fox's thesis (p. 118) to get samples from the joint posterior of the hidden states given the observed states. The idea is to first compute the backward messages, then sequentially sample hidden states in a forward direction.

```{r}
msg_b<-comp_bk_msg(transition,elps) #compute backward messages, logscale by default
pi0<-log(get_stationary_dist(transition))
nRep<-10
zs<-replicate(nRep,sample_hidden_states(msg_b,elps,transition,pi0))
# zs is now a Tmax by nRep matrix
# find which state most likely in samples at each time step
zs_pred<-apply(zs,1,function(q){which.max(tabulate(q))})
zs_pred<-hidden_int2char(zs_pred,categs)
table(z,zs_pred)
```

The sampled paths always chose the true path. This is a "unit test" since the data were so clean. Let's try again with some data where the hidden state is not so obvious given the observed state.

```{r}
```