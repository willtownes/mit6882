---
title: "Hidden Markov Model"
author: "Will Townes"
date: "April 11, 2016"
output: html_document
---

```{r}
library(expm)
library(ggplot2)
source("util.R") #forward/backward message calculations
```

Let $z_t$ denote the hidden mode sequence. Each $z_t$ is a categorical random variable pointing to one of the L possible modes. Conditional on $z_t$, we observe the continuous variable $x_t$. The first model we examine sets L=2 with the hidden states "Big" ("B") and "Small" ("S"). We set the transition probabilities as follows:
$$P(z_t="B"|z_{t-1}="B") = .99$$
$$P(z_t="S"|z_{t-1}="B") = .01$$
$$P(z_t="B"|z_{t-1}="S") = .05$$
$$P(z_t="S"|z_{t-1}="S") = .95$$
The emission probabilities are set as follows:
$$P(x_t|z_t="B")=\mathcal{N}(5,2)$$
$$P(x_t|z_t="S")=\mathcal{N}(-5,2)$$

The first step is to create some simulated data from this HMM.

```{r}
# transition matrix, rows sum to one
transition<-matrix(c(.99,.05,.01,.95),nrow=2) #premultiply by indicator to get new vals
stationary_dist<-eigen(t(transition))$vectors[,1]
stationary_dist/sum(stationary_dist)
transition%^%100
#reveals stationary distribution is c(.83,.17)
categs<-c("B","S")
rownames(transition)<-colnames(transition)<-categs
L<-nrow(transition) #2
Tmax<-1000
z<-rep(NA,Tmax)
z[1]<-"S"
for(t in 2:Tmax){
  z[t]<-sample(categs,1,prob=transition[z[t-1],])
}
table(z)/Tmax #should resemble stationary distribution
mu_vals_map<-list(B=5,S=-5)
x<-sapply(z,function(q){rnorm(1,mean=mu_vals_map[[q]],sd=sqrt(2))})
plotdat<-data.frame(index=1:Tmax,x=x,z=z)
ggplot(data=plotdat)+geom_point(aes(x=index,y=x,colour=z))
```

The simulated data shows the correct pattern matching with the stationary distribution desired. We will now use the forward-backward algorithm to infer the hidden states and see if it matches with the true hidden states.

```{r}
#compute all emission log-probabilities
elps<-vapply(categs,function(q){dnorm(x,mean=mu_vals_map[[q]],sd=sqrt(2),log=TRUE)},FUN.VALUE=rep(0.0,Tmax))
# prev. line is a Tmax by L matrix
# each row is one time step.
# values are on the log-scale
msg_gam<-forward_backward(transition,elps)
#find predicted states most likely, doesn't require normalization!
zpreds<-max.col(msg_gam)
zpreds<-vapply(zpreds,function(q){categs[q]},FUN.VALUE="S")
table(zpreds,z)
```
We see in the above table that everything was correctly classified by the Forward-Backward Algorithm!