---
title: "Linear Dynamical System"
author: "Will Townes"
date: "April 21, 2016"
output: html_document
---

```{r}
library(mvtnorm)
library(ggplot2)
```

### Generating Fake Data

The linear dynamical system model is given by the following recursive equations:
$$x_t|x_{t-1}\sim\mathcal{N}(A_t x_{t-1},\Sigma_t)$$
$$y_t|x_t\sim\mathcal{N}(C_t x_t,R_t)$$

In Fox et al, they set $C_t=C$ and $R_t=R$ to be constant across time and without loss of generality force $C=[I_d,0]$ where $d$ is the dimensionality of $y_t$. Hence the key parameters that switch are $A_t,\Sigma_t$ and the key parameter that is constant for all time is $R$. 

For the purposes of visualization, we produce data from a projectile trajectory. We follow the discussion in Section 1.3.2 in *Bayesian Time Series Models* by Barber et al. According to a discretization of Newton's Laws of motion, acceleration is constant. Let $q(t)$ be the two-vector of position, $v(t)$ be the velocity vector, and $a=(0,-9.8)$ be the acceleration. If a projectile is launched from the origin $q_0=(0,0)$ with initial velocity $v_0=(10,10)$ (ie the initial angle is $\theta=\pi/4$, we expect it to follow a parabolic path to a maximum height of 
$$y_{max} = \frac{\Vert v_0 \Vert^2\sin^2(\theta)}{2\Vert a\Vert}$$

A discretized version of the trajectory is:
$$q(t+k) = q(t) + k v(t) + \frac{k^2}{2} a$$
$$v(t+k) = v(t) + k a(t) $$
Where $k$ is the time gap between observations (assumed to be constant for simplicity).
Let $x(t) = (q(t),v(t))$ (a 4-vector). We can write $x(t) = A x(t-1) + B$ where
$$A = \begin{pmatrix} I & kI\\0 & I\end{pmatrix}$$
$$B = \begin{pmatrix} \frac{1}{2}k^2 Ia\\ kIa\end{pmatrix}$$
Hence
$$(x_t|x_{t-1})\sim\mathcal{N}\left(Ax_{t-1}+B,\Sigma\right)$$
with $\Sigma$ representing random perturbations in motion due to the wind. We set $\Sigma$ to a a diagonal matrix with diagonal elements $(\sigma^2_1,\sigma^2_1,\sigma^2_2,\sigma^2_2)$. This indicates the noise in the velocity part is different than the position part, but the noise is the same in any spatial direction. This is a linear dynamical system. Assume we observe only a noisy version of the position, $y(t)$. Hence, the observation model is
$$(y_t|x_t)\sim\mathcal{N}\left(Cx_t,R\right)$$
Where $C = (I,0)$ (a 2x4 matrix) and $R$ is measurement noise. We can assume $R$ is an isotropic noise model for simplicity (ie, that it is a scalar times the identity). The simulated data is shown below

```{r}
#inputs
meas_err_sd<-200
pos_noise_sd<-10
velo_noise_sd<-40
k<-5 #number of seconds between observations
init_pos<-c(0,0)
init_velo<-c(250,250)
Tmax<-60 #number of seconds to run the simulation

iter_max<-floor(Tmax/k)
a<-c(0,-9.8) #acceleration
x<-matrix(NA,nrow=iter_max,ncol=4) #position and velocity
colnames(x)<-c("pos_horiz","pos_vert","velo_horiz","velo_vert")
y<-matrix(NA,nrow=iter_max,ncol=2) #observed position
colnames(y)<-c("pos_horiz","pos_vert")
x[1,]<-c(init_pos,init_velo)
y[1,]<-init_pos
A<-diag(4)
A[1:2,3:4]<-k*diag(2)
B<-rbind(.5*k^2*diag(2),k*diag(2))%*%a
C<-cbind(diag(2),matrix(0,nrow=2,ncol=2))
Sigma<-diag(c(rep(pos_noise_sd^2,2),rep(velo_noise_sd^2,2)))
R<-meas_err_sd^2*diag(2)

for(t in 2:iter_max){
  x[t,]<-rmvnorm(1,drop(A%*%x[(t-1),]+B),Sigma)
  y[t,]<-rmvnorm(1,drop(C%*%x[t,]),R)
}
plot(x[,1],x[,2],type="l")
points(y[,1],y[,2])
```

The curve indicates the true (discretized) trajectory, the circles represent the noisy observations. The task now is to try to infer the curve given only the circles. We apply the backwards message passing, forwards sampling approach of Fox et al. The backwards message passing algorithm is Algorithm 19 in Fox's thesis. Key functions defined below:

```{r}
rmvnorm_info<-function(n,theta,Lambda){
  # returns n by d matrix. n=number of replicates
  # d=dimension of multivariate normal
  # theta= offset parameter
  # Lambda = information matrix
  # special case of n=1, returns a vector, otherwise, a matrix
  S<-solve(Lambda)
  m<-solve(Lambda,theta)
  if(n>1){
    return(rmvnorm(n,m,S))
  } else {
    return(drop(rmvnorm(n,m,S)))
  }
}

backward_kalman_msgs<-function(y,z,pars,C,R){
  # inputs:
  # y is a matrix where each row is an observation
  # z is a vector of mode indicators. For time-invarying dynamics, set z=rep(1,ncol(y))
  # pars is a list, one element for each unique mode (==length(unique(z)))
  # pars[[i]] is a list containing time-varying parameters
  # "A" is transition matrix for linear dynamical system (LDS)
  # "B" is additive term for transition in LDS
  # "Sigma" is noise for transition in LDS
  # C and R are (time-invariant) parameters for observation model
  Tmax<-nrow(y)
  #D<-ncol(y)
  eye<-diag(ncol(A))
  #note if u=chol(R) then R=u'u, NOT uu'.
  #R^{-1} = u^{-1}u^{-T}
  #implementing Algorithm 19 from Emily Fox MIT dissertation
  #U<-chol(R)
  #UtiC<-solve(t(U),C)
  #CtRiC<-t(UtiC)%*%UtiC
  CtRiC<-t(C)%*%solve(R,C)
  #initialize messages
  Lambda_tt<-lapply(1:Tmax,function(x){CtRiC})
  theta_tt<-t(apply(y,1,function(yt){t(UtiC)%*%solve(t(U),yt)}))
  for(t in Tmax:1){
    Lambda<-Lambda_tt[[t]]
    par_t<-pars[[z[t]]]
    A<-par_t[["A"]]
    B<-par_t[["B"]]
    Sigma<-par_t[["Sigma"]]
    Si<-solve(Sigma)
    Jt<-t(solve(Lambda+Si,Lambda))
    Lt<-eye - Jt
    Lam<-t(A)%*%(Lt%*%Lambda%*%t(Lt)+Jt%*%solve(Sigma,t(Jt)))%*%A
    theta<-t(A)%*%Lt%*%(theta_tt[t,] - Lambda%*%B)
    if(t>1){
      Lambda_tt[[t-1]]<-Lam+CtRiC #or Lam+Lambda_tt[[t-1]]
      theta_tt[t-1,]<-theta+theta_tt[t-1,]
    } else {
      Lambda_00<-Lam
      theta_00<-theta
    }
  }
  res<-list()
  res[["info_matrix_msgs"]]<-Lambda_tt #latent state dim x latent state dim
  res[["offset_msgs"]]<-theta_tt #Tmax by dimension of latent state
  res[["info_matrix_init"]]<-Lambda_00
  res[["offset_msg_init"]]<-theta_00
  return(res)
}

fwd_kalman_sample<-function(z,pars,Lambda_00,Lambda_tt,theta_00,theta_tt,xs=NULL){
  # z,pars are as described in backward_kalman_msgs
  # remaining parameters are form output of backward_kalman_msgs:
  # Lambda_tt is list of all the information matrix messages from backward Kalman Filter
  # Lambda_00 is initial information matrix
  # theta_00 is initial offset parameter vector
  # theta_tt is matrix where row t is offset parameter for step t in the time series
  # can save time by pre-allocating result matrix xs
  Tmax<-length(z)
  if(is.null(xs)){
    D<-ncol(pars[[1]][["Sigma"]])
    xs<-matrix(NA,nrow=Tmax,ncol=D)
  }
  xs0<-rmvnorm_info(1,theta_00,Lambda_00) #initialize x0, not sure if right
  for(t in 1:Tmax){
    par_t<-pars[[z[t]]]
    A<-par_t[["A"]]
    B<-par_t[["B"]]
    Sigma<-par_t[["Sigma"]]
    info_mat<-solve(Sigma)+Lambda_tt[[t]]
    if(t==1){
      offset<-solve(Sigma,A%*%xs0+B)+theta_tt[t,]
    } else {
      offset<-solve(Sigma,A%*%xs[t-1,]+B)+theta_tt[t,]
    }
    xs[t,]<-rmvnorm_info(1,offset,info_mat)
  }
  return(xs)
}
rLDS<-function(n,y,z,pars,C,R){
  # convenience wrapper for combining backward_kalman_msgs with fwd_kalman_sample
  # n is number of desired samples
  # each sample is a matrix
  # returns a list of length n
  # special case n=1, returns a single matrix
  # dim(matrix) is nrow=number of steps in time series, ncol=dim(latent state)
  msg_b<-backward_kalman_msgs(y,z,pars,C,R)
  L_tt<-msg_b[["info_matrix_msgs"]] #latent state dim x latent state dim
  th_tt<-msg_b[["offset_msgs"]] #Tmax by dimension of latent state
  L_00<-msg_b[["info_matrix_init"]]
  th_00<-msg_b[["offset_msg_init"]]
  res<-replicate(n,fwd_kalman_sample(z,pars,L_00,L_tt,th_00,th_tt),simplify=FALSE)
  return(res)
}
rLDS_melt<-function(LDS_samples,cnames=NULL){
  # takes output of rLDS and converts into a giant data frame
  # facilitates making plots with ggplot()
  #for(n in 1:length(LDS_samples)){
  #  LDS_samples[[n]]<-cbind(LDS_samples[[n]],"id"=n)
  #}
  Tmax<-nrow(LDS_samples[[1]])
  D<-ncol(LDS_samples[[1]])
  n<-length(LDS_samples)
  res<-do.call("rbind",LDS_samples)
  res<-cbind(res,"id"=rep(1:n,each=Tmax))
  if(is.null(cnames)){
    return(res)
  } else {
    colnames(res)<-c(cnames,"id")
    return(res)
  }
}
```

Now we test out the functions

```{r}
pars<-list()
pars[[1]]<-list("A"=A,"B"=B,"Sigma"=Sigma)
z<-rep(1,nrow(y)) #default to only one mode
xs<-rLDS(10,y,z,pars,C,R) #ten samples from the joint posterior
# munge res into ggplot compatible format
xs<-rLDS_melt(xs,colnames(x))
#plot(xs[,1],xs[,2],col="red")
#lines(x[,1],x[,2],col="blue")
#points(y[,1],y[,2],col="green")
```


