---
title: "Dirichlet Process"
author: "Will Townes"
date: "March 19, 2016"
output: html_document
---

```{r}
library(mvtnorm)
library(ggplot2)
library(car) #for plotting ellipses
#library(plyr)
#library(MCMCpack)
```

First we generate some fake data from a mixture model of three two-dimensional Gaussians. In the first simulation, we assume isotropic, known variance for all clusters.

```{r}
genCluster<-function(n,id,mu=c(0,0),sigma=diag(2)){
  d<-data.frame(rmvnorm(n,mean=mu,sigma=sigma))
  d$id<-id
  return(d)
}

#upper left- isotropic
red<-genCluster(20,"1",mu=c(-3,3)) #sigma=cbind(c(2,-.5),c(-.5,2))
#upper right- stretched horizontally
blue<-genCluster(20,"2",c(3,3),sigma=matrix(c(1.5,0,0,.5),nrow=2))
#lower right- positive correlation
green<-genCluster(20,"3",c(3,-3),sigma=matrix(c(1,.8,.8,1),nrow=2))
#lower left- negative correlation
black<-genCluster(20,"4",c(-3,-3),sigma=matrix(c(1,-.8,-.8,1),nrow=2))
dat<-rbind(red,blue,green,black)
ggplot(data=dat,aes(x=X1,y=X2,color=id))+geom_point(size=3)+theme_bw()+geom_density2d(h=4)+xlim(-6,6)+ylim(-6,6)
ggplot(data=dat,aes(x=X1,y=X2))+geom_point(size=3)+theme_bw()+xlim(-6,6)+ylim(-6,6)
```

### Sampler from a Dirichlet Process

```{r}
# rstick<-function(K,alpha){
#   # returns a truncated stick breaking vector of weights
#   # alpha is concentration parameter
#   # K is the number of weights, set K to a large value to more closely approximate the true stick breaking distribution
#   # large alpha-> few clusters
#   # small alpha-> many clusters
#   betas<-rbeta(K,1,alpha)
#   betas[K]<-1 #truncation part
#   lbetas<-log(betas)
#   log_cumsum_1minus_betas<-c(0,cumsum(log(1-betas))[1:K-1])
#   return(exp(lbetas+log_cumsum_1minus_betas))
# }
# # test this function
# stopifnot(sum(rstick(1,10))==1)

rstick<-function(alpha,categ_counts){
  # returns a truncated stick breaking vector of weights
  # alpha is (prior) concentration parameter
  # large alpha-> few clusters
  # small alpha-> many clusters
  # categ_counts is a vector of sufficient statistics
  # categ_counts[h] is number of data points observed in category "h"
  # categ_counts = sum_{i=1}^n I(Z_i=h)
  # set categ_counts=rep(0,K) to sample from the prior.
  K<-length(categ_counts)
  #compute parameter for second part of stick breaking betas
  #ss_gt = sum_{i=1}^n I(Z_i>h) for each of the categoris h=1...K
  ss_gt<-sum(categ_counts)-cumsum(categ_counts)
  betas<-rbeta(K,1+categ_counts,alpha+ss_gt)
  betas[K]<-1 #truncation part
  lbetas<-log(betas)
  log_cumsum_1minus_betas<-c(0,cumsum(log(1-betas))[1:K-1])
  return(exp(lbetas+log_cumsum_1minus_betas))
}
# test this function
stopifnot(sum(rstick_post(1,c(5,3,8))==1))

rdp_prior<-function(K,alpha,H){
  # return samples from a dirichlet process prior with concentration parameter alpha and base measure H.
  # uses stick breaking method rather than chinese restaurant process
  # truncates stick breaking after K clusters
  # calling H(K) should produce K random samples from distribution H
  # returns a list of probabilities and atoms
  # ie, one sample from rdp is a probability measure over the atoms
  pi_vals<-rstick(alpha,rep(0,K))
  atoms<-H(K)
  return(list(probs=pi_vals,atoms=atoms))
}

visualize_dp_1d<-function(alpha,H=rnorm,H_density=dnorm,K=100){
  # handy function for visualizing 1-dimensional DPs
  # "H_density" should be the density version of "H" for plotting
  test_dp<-data.frame(rdp_prior(K,alpha,H))
  ggbase<-ggplot(data=test_dp,aes(x=atoms,ymax=probs,ymin=0))+geom_linerange(size=2)+stat_function(fun=H_density,linetype=2)+ylab("probability density or mass")+theme_bw()
  return(ggbase)
}

visualize_dp_1d(1)+ggtitle("DP with Normal(0,1) Base Measure, alpha=1")
visualize_dp_1d(10)+ggtitle("DP with Normal(0,1) Base Measure, alpha=10")
```

### Sampler from Normal-Wishart Distribution
We now define a sampler function from a normal-wishart joint distribution. This is useful as a prior for normally distributed data and comes in handy in the gibbs sampler.

```{r}
rNW<-function(m,k,df,precision_matrix){
  # return a single sample from the normal-wishart distribution
  # m is mean parameter
  # k is related to "sample size" for mean. Large value==high confidence in m
  # precision_matrix is inverse of the covariance matrix
  # df is "sample size" for covariance matrix. Large value==high confidence
  # probability model:
  # Lambda ~ Wishart(precision_matrix,df)
  # equivalently, Sigma=Lambda^{-1}~ Inverse Wishart(S,nu=df+D+1) 
  # where D is dimensionality
  # mu|Sigma ~ Normal(m,(1/k)*S)
  #Lambda0<-solve(S)
  #stopifnot(df>0)
  D<-length(m)
  #stopifnot(all(dim(precision_matrix)==D))
  nu<-df+D+1
  Lambda<-drop(rWishart(1,df,precision_matrix))
  Sigma<-solve(Lambda)
  mean_sim<-drop(rmvnorm(1,m,Sigma/k))
  return(list(mean=mean_sim,precision=Lambda,cov=Sigma))
}
# test the function using strongly informative prior
nsim<-1000
vals<-replicate(nsim,rNW(c(5,-5),10,10,matrix(c(1,.5,.5,1),nrow=2)),simplify=FALSE)
rowMeans(sapply(vals,function(x){x$mean})) #should be (5,-5)
#empirical mean of precision matrices should be matrix(10,5,5,10)
Reduce("+",lapply(vals,function(x){x$precision}))/nsim
```

### Sampler from Dirichlet Process Mixture of Multivariate Gaussians
Next let's try simulating individual data points from a dirichlet process gaussian mixture model. The "N" data points come from an infinite mixture of gaussians in the following way. An infinite number of (mean, covariance) parameters are sampled from the base distribution (in reality truncated to K sets of parameters). An infinite number (actually, truncated to K) of weights are sampled from the stick breaking process. Then, we sample "N" times with replacement from the sets of (mean,covariance) parameters. Finally, a single data point is simulated from each of these sets. Conditional on cluster membership, a point is independently drawn from a gaussian.

```{r}
dp_gmm_prior<-function(N,K,alpha,m,k,df,precision_matrix,verbose=FALSE){
  # return N samples from DP mixture of gaussians
  # K is truncation level of stick breaking process (upper bound on #clusters)
  # alpha is concentration parameter 
  #gaussian base measure hyperparams m,k,df, and precision matrix defined in rNW function
  probs<-rstick(alpha,rep(0,K))
  params_unique<-replicate(K,rNW(m,k,df,precision_matrix),simplify=FALSE)
  params<-sample(params_unique,N,replace=TRUE,prob=probs)
  vals<-vapply(params,function(x){rmvnorm(1,x$mean,x$cov)},rep(0.0,length(m)))
  if(verbose) print(paste("Number of clusters=",length(unique(params))))
  attr(vals,"nclust")<-length(unique(params))
  return(t(vals))
}
vals<-dp_gmm_prior(1000,100,1,c(5,-5),.1,3,matrix(c(1,.5,.5,1),nrow=2),verbose=TRUE)
#colMeans(vals) #should be about c(5,-5)
plot(vals)
hist(vals[,1],probability=TRUE) #marginal of simulated data
curve(dnorm(x,mean=5,sd=3),from=-20,to=40,add=TRUE) #compare to base meas.
hist(vals[,2],probability=TRUE)
curve(dnorm(x,mean=-5,sd=3),from=-40,to=20,add=TRUE)
ggplot(data=data.frame(vals),aes(x=X1,y=X2))+geom_point(size=.5)+geom_density2d()+theme_bw()
#ggsave("DP_GMM.pdf")
```

### Gibbs Sampler for Inferring Model Parameters

From Murphy book, we know that if the data $x_i$ are iid multivariate normally distributed with parameters $\mu$ and $\Sigma$ and if $(\mu,\Sigma)$ are jointly distributed as normal-inverse wishart having parameters $(m_0,\kappa_0,\nu_0,S_0)$ then the posterior for $(\mu,\Sigma)|x_1,..x_n$ is normal-inverse wishart with parameters:
$$m_n = \frac{\kappa_0 m_0 + \sum_{i=1}^N x_i}{\kappa_N}$$
$$\kappa_N = \kappa_0+N$$
$$\nu_N = \nu_0+N$$
$$S_N = \left(S_0^{-1}+S+\kappa_0 m_0 m_0' - \kappa_N m_N m_N'\right)^{-1}$$
$$S = \sum_{i=1}^N x_i x_i'$$

The Gibbs sampler alternates between sampling from the full conditionals of $\mu,\Sigma$ for each cluster (conditional on cluster membership, the prior is the base measure and the data are the points assigned to that cluster), and sampling from the DP posterior given the model parameters. 

The blocked sampler is described in the Hjort et al book, in particular Ch. 7: "Nonparametric Bayes Applications to Biostatistics" by David Dunson. First, we allocate each data point to one of the $L$ possible clusters (L is the truncation level). Let $Z_i$ be the indicator of cluster membership for the $i^{th}$ data point. Then the full conditional for each $Z_i$ is a categorical distribution. The prior probability of $P(Z_i=h)$ is designated as $\pi_h$, which comes from the stick weights. The full conditional probability of $Z_i=h$ is given by:
$$P(Z_i=h|rest) = \frac{\pi_h N(x_i|z_i=h,\mu_h,\Sigma_h)}{\sum_{z_i=r}^L \pi_r N(x_i|z_i=r,\mu_r,\Sigma_r)}$$
Note that the full distribution is then given by:
$$P(z_i|rest) = $$
for $h=1,...L$.

```{r}
# inputs:
# y is a matrix where each row is a data point
# alpha is the concentration parameter
# H is the base distribution
alpha<-1
y<-as.matrix(dat[,1:2])
k0<-.01 #diffuse prior for means
D<-ncol(y)
nu0<-D+2 #diffuse prior for covariance matrix
# set other hyperparameters based on empirical bayes approach
ymean<-colMeans(y)
ycov<-cov(y)
m0<-ymean
S0_inv<-solve(ycov)
# set base measure also by empirical bayes if not specified
if(is.null(H)) H<-function(n){rmvnorm(n,mean=ymean,sigma=S0)}
#update hyperparameters based on data
N<-nrow(y)
kn<-k0+N
mn <- (k0*m0+colSums(y))/kn
nun<-nu0+N
Sn<-solve(S0_inv+t(y)%*%y+k0*(m0 %o% m0) - kn*(mn %o% mn))
ggplot(data=dat)+geom_point(aes(x=X1,y=X2,color=id),size=2)
ellipse(mn,Sn,radius=1,grid=FALSE,add=TRUE)
```