\documentclass[12pt]{article}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\newcommand{\E}{\mathbb{E}}

\begin{document}
\title{MIT 6.882 Final Project}
\author{Jeremiah Zhe Liu,  Will Townes}
\maketitle

\section{Block Sampling for Linear Dynamical System}
\subsection{Forward Kalman Filter}
Let $x_t$ represent the hidden (continuous) state and let $y_t$ represent the noisy observation. Then the linear dynamical system of interest has the following probability model:

\begin{align*}
p(x_t|x_{t-1}) &= \mathcal{N}(A_t x_{t-1}+B_t,\Sigma_t)\\
p(y_t|x_t) &= \mathcal{N}(C x_t,R)
\end{align*}

Following Fox (see thesis section 2.7.5), we set $C = [I_d,0]$ where $d$ is the dimensionality of $y_t$. The key switching parameters are $A_t, B_t$, and $\Sigma_t$ and the key constant parameter is $R$. Note that our model is more general than that of Fox since we allow the presence of the $B_t$ parameter while she sets it to zero. Assuming these dynamical parameters are known, we can use a variant of the Kalman Filter to sample from the posterior of all the $x_t$ states given the observed $y_t$ states. The idea is to first compute backward messages and then sample in a forward pass. First, we derive the recursion for the forward messages.

\begin{align*}
\alpha_{t+1}(x_{t+1}) &= \left[\int p(x_{t+1}|x_t)\alpha_t(x_t)dx_t\right] p(y_{t+1}|x_{t+1})
\end{align*}

Ignoring normalizing constants, the integrand depends on the following quantities
\begin{align*}
p(x_{t+1}|x_t)&\propto \exp\left\{-\frac{1}{2}(x_{t+1}-Ax_t-B)'\Sigma^{-1}(x_{t+1}-Ax_t-B)\right\}\\
&\propto \exp\left\{-\frac{1}{2}\begin{pmatrix}x_{t+1}\\x_t\end{pmatrix}'
    \begin{pmatrix}\Sigma^{-1} & -\Sigma^{-1}A\\
    -A'\Sigma^{-1} & A'\Sigma^{-1}A\end{pmatrix}\begin{pmatrix}x_{t+1}\\x_t\end{pmatrix}
    + \begin{pmatrix}\Sigma^{-1}B\\-A'\Sigma^{-1}B\end{pmatrix}'\begin{pmatrix}x_{t+1}\\x_t\end{pmatrix}\right\}
\end{align*}
Assume $\alpha_t(x_t)$ is a known Gaussian density function with offset $\theta^f_{t|t}$ and information matrix $\Lambda^f_{t|t}$. Then Fox shows that
\[\alpha_t(x_t)\propto\exp\left\{-\frac{1}{2}\begin{pmatrix}x_{t+1}\\x_t\end{pmatrix}'
    \begin{pmatrix}0 & 0\\
    0 & \Lambda^f_{t|t}\end{pmatrix}\begin{pmatrix}x_{t+1}\\x_t\end{pmatrix}
    + \begin{pmatrix}0\\ \theta^f_{t|t}\end{pmatrix}'\begin{pmatrix}x_{t+1}\\x_t\end{pmatrix}\right\}\]
The combined density in the integrand is then given by
\begin{align*}
p(x_{t+1}|x_t)\alpha_t(x_t)&\propto \exp\left\{-\frac{1}{2}\begin{pmatrix}x_{t+1}\\x_t\end{pmatrix}'
    \begin{pmatrix}\Sigma^{-1} & -\Sigma^{-1}A\\
    -A'\Sigma^{-1} & A'\Sigma^{-1}A + \Lambda^f_{t|t}\end{pmatrix}\begin{pmatrix}x_{t+1}\\x_t\end{pmatrix}\right. \ldots\\
    &~~\ldots+ \left. \begin{pmatrix}\Sigma^{-1}B\\ \theta^f_{t|t}-A'\Sigma^{-1}B\end{pmatrix}'\begin{pmatrix}x_{t+1}\\x_t\end{pmatrix}\right\}
\end{align*}
We now have the joint distribution of $(x_{t+1},x_t)$ which is in the form of a blocked bivariate Gaussian. The marginal distribution of $x_{t+1}$ is obtained by integrating out $x_t$ using a standard identity:
\[\int\mathcal{N}^{-1}\left(\begin{pmatrix}x_1\\ x_2\end{pmatrix} ; \begin{pmatrix}\theta_1\\ \theta_2\end{pmatrix}, \begin{pmatrix}\Lambda_{11} & \Lambda_{12} \\ \Lambda_{21}& \Lambda_{22}\end{pmatrix}\right)dx_2 = \mathcal{N}^{-1}(x_1;\theta_1-\Lambda_{12}\Lambda_{22}^{-1}\theta_2, ~ \Lambda_{11} - \Lambda_{12}\Lambda_{22}^{-1}\Lambda_{21})\]

Therefore,
\[\int p(x_{t+1}|x_t)\alpha_t(x_t)dx_t \propto \mathcal{N}^{-1}(x_{t+1};\theta_{t,t+1},\Lambda_{t,t+1})\]
where
\begin{align*}
\theta_{t,t+1} &=\Sigma^{-1}B + \Sigma^{-1}A(A'\Sigma^{-1}A+\Lambda^f_{t|t})^{-1}(\theta^f_{t|t} - A'\Sigma^{-1}B)\\
\Lambda_{t,t+1} &= \Sigma^{-1} - \Sigma^{-1}A(A'\Sigma^{-1}A+\Lambda^f_{t|t})^{-1}A'\Sigma^{-1}
\end{align*}
Notably, our offset term is different from Fox due to the nonzero $B$ but the information matrix is the same. The above update equations can be simplified if $A$ is invertible (cf Fox Algorithm 3). Additionally, it is desirable to enforce symmetry in computing $\Lambda_{t,t+1}$. Set $M_t = (A')^{-1}\Lambda^f_{t|t}A^{-1}$ and $J_t = M_t(M_t+\Sigma^{-1})^{-1}$. Note that $M_t' = M_t$. Then,
\begin{align*}
\Lambda_{t,t+1} &= \Sigma^{-1}\left(I - \left(\Sigma^{-1}+(A')^{-1}\Lambda^f_{t|t}A^{-1}\right)^{-1}\Sigma^{-1}\right) = \Sigma^{-1}\left(I - \left(\Sigma^{-1}+M_t\right)^{-1}\Sigma^{-1}\right)\\
&=\Sigma^{-1}\left(\Sigma^{-1}+M_t\right)^{-1}M_t = \Sigma^{-1}J_t'
\end{align*}
This is equivalent to Fox's Algorithm 3 formula, as shown below:
\begin{align*}
\Lambda_{t,t+1} &= (I-J_t)M_t(I-J_t)' + J_t\Sigma^{-1}J_t'\\
 &= (I-J_t)M_t(I-J_t)' + J_t(\Lambda_{t,t+1})\\
 (I-J_t)\Lambda_{t,t+1} &= (I-J_t)M_t(I-J_t')\\
 \Lambda_{t,t+1} &= M_t(I-J_t') = M_t-M_tJ_t'\\
 \Sigma^{-1}J_t' &= M_t-M_tJ_t'\\
 (\Sigma^{-1} + M_t)J_t' &= M_t\\
 (\Sigma^{-1}+M_t)(\Sigma^{-1}+M_t)^{-1}M_t &= M_t
\end{align*}
Fox's formula is better numerically since it automatically enforces symmetry. By a similar argument, we have a simplified version of the offset parameter:
\begin{align*}
\theta^f_{t,t+1} &= \Sigma^{-1}\left(B+\left(\Sigma^{-1}+M_t\right)^{-1}\left((A')^{-1}\theta^f_{t|t} - \Sigma^{-1}B\right)\right)\\
&= \Sigma^{-1}\left(\left(I-(\Sigma^{-1}+M_t)^{-1}\Sigma^{-1}\right)B + \left(\Sigma^{-1}+M_t\right)^{-1}(A')^{-1}\theta^f_{t|t}\right)\\
&= \Sigma^{-1}J_t'B + \Sigma^{-1}(\Sigma^{-1}+M_t)^{-1}(A')^{-1}\theta^f_{t|t}\\
&= \Lambda_{t,t+1}B + (I-J_t)(A')^{-1}\theta^f_{t|t}
\end{align*}
This reduces to Fox's formula when $B=\mathbf{0}$ as expected.

The likelihood term is the same as in Fox:
\[p(y_{t+1}|x_{t+1})\propto\exp\left\{-\frac{1}{2} x_{t+1}'C'R^{-1}Cx_{t+1} + x_{t+1}'C'R^{-1}y_{t+1}\right\}\]
The combined density is then given by
\[\alpha_{t+1}(x_{t+1})\propto\exp\left\{-\frac{1}{2}x_{t+1}'\left(\Lambda_{t,t+1}+C'R^{-1}C\right)x_{t+1} + x_{t+1}'\left(\theta_{t,t+1}+C'R^{-1}y_{t+1}\right)\right\}\]
Therefore the updated filtering information and offset parameters at step (t+1) are:
\begin{align*}
\theta^f_{t+1|t+1} &= \theta_{t,t+1} + C'R^{-1}y_{t+1}\\
%&=\Sigma^{-1}B + \Sigma^{-1}A(A'\Sigma^{-1}A+\Lambda^f_{t|t})^{-1}(\theta^f_{t|t} - A'\Sigma^{-1}B) + C'R^{-1}y_{t+1}\\
\Lambda^f_{t+1|t+1} &= \Lambda_{t,t+1} + C'R^{-1}C %\\
%&=\Sigma^{-1} - \Sigma^{-1}A(A'\Sigma^{-1}A+\Lambda^f_{t|t})^{-1}A'\Sigma^{-1}+ C'R^{-1}C
\end{align*}
Filtered estimates of $x_{t}|y_{1:t}$ can be obtained from $\E[x_{t}|y_{1:t}] = \hat{x}_{t|t} = \left(\Lambda^f_{t|t}\right)^{-1}\theta^f_{t|t}$ or sampled from the updated density $x_{t}|y_{1:t}\sim\mathcal{N}\left(\hat{x}_{t|t}~,~\left(\Lambda^f_{t|t}\right)^{-1}\right)$

\subsection{Backward Kalman Filter}

This section follows closely with Fox Appendix D.2. To perform smoothing or sampling based on the joint distribution of the hidden states given the observed states (rather than just the filtered distribution), we also need to compute backward messages, defined as:
\[m_{t,t-1}(x_{t-1}) = p(y_{t:T}|x_{t-1})\]
These are similar to the $\beta_{t-1}$ messages that would be computed in the backward part of the forward-backward algorithm. If we already know $m_{t+1,t}(x_t)\sim\mathcal{N}^{-1}(x_t;~\theta^b_{t+1,t},~\Lambda^b_{t+1,t})$, then
\[m_{t,t-1} \propto \int p(x_t|x_{t-1})p(y_t|x_t)m_{t+1,t}(x_t)dx_t\]
The components of the integrand can be expressed as:
\begin{align*}
p(x_t|x_{t-1})&\propto\exp\left\{-\frac{1}{2}\begin{pmatrix}x_{t-1}\\x_t\end{pmatrix}'\begin{pmatrix}A'\Sigma^{-1}A & -A'\Sigma^{-1}\\-\Sigma^{-1}A & \Sigma^{-1}\end{pmatrix}\begin{pmatrix}x_{t-1}\\x_t\end{pmatrix} + \begin{pmatrix}x_{t-1}\\x_t\end{pmatrix}'\begin{pmatrix}-A'\Sigma^{-1}B\\ \Sigma^{-1}B\end{pmatrix}\right\}\\
p(y_t|x_t)&\propto\exp\left\{-\frac{1}{2}\begin{pmatrix}x_{t-1}\\x_t\end{pmatrix}'\begin{pmatrix}0 & 0\\0 & C'R^{-1}C\end{pmatrix}\begin{pmatrix}x_{t-1}\\x_t\end{pmatrix} + \begin{pmatrix}x_{t-1}\\x_t\end{pmatrix}'\begin{pmatrix}0\\ C'R^{-1}y_t\end{pmatrix}\right\}\\
m_{t+1,t}(x_t)&\propto\exp\left\{-\frac{1}{2}\begin{pmatrix}x_{t-1}\\x_t\end{pmatrix}'\begin{pmatrix}0 & 0\\0 & \Lambda^b_{t+1,t}\end{pmatrix}\begin{pmatrix}x_{t-1}\\x_t\end{pmatrix} + \begin{pmatrix}x_{t-1}\\x_t\end{pmatrix}'\begin{pmatrix}0\\ \theta^b_{t+1,t}\end{pmatrix}\right\}
\end{align*}
Combining these together, the integrand becomes:
\begin{align*}
p(x_t|x_{t-1})p(y_t|x_t)m_{t+1,t}(x_t)&\propto\exp\left\{-\frac{1}{2}\begin{pmatrix}x_{t-1}\\x_t\end{pmatrix}'\begin{pmatrix}A'\Sigma^{-1}A & -A'\Sigma^{-1}\\-\Sigma^{-1}A & \Sigma^{-1}+C'R^{-1}C+\Lambda^b_{t+1,t}\end{pmatrix}\begin{pmatrix}x_{t-1}\\x_t\end{pmatrix}\right.\ldots\\
&\left. ~\ldots + \begin{pmatrix}x_{t-1}\\x_t\end{pmatrix}'\begin{pmatrix}-A'\Sigma^{-1}B\\ \Sigma^{-1}B+C'R^{-1}y_t+\theta^b_{t+1,t}\end{pmatrix}\right\}
\end{align*}
Applying the Gaussian marginalization identity from the previous section to integrate out $x_t$, we obtain
\[m_{t,t-1}\propto\mathcal{N}^{-1}(x_{t-1}~;~\theta^b_{t,t-1}~,~\Lambda^b_{t,t-1})\]
where
\begin{align*}
\Lambda^b_{t,t-1} &= A'\Sigma^{-1}A - A'\Sigma^{-1}(\Sigma^{-1}+C'R^{-1}C+\Lambda^b_{t+1,t})^{-1}\Sigma^{-1}A\\
\theta^b_{t,t-1} &= -A'\Sigma^{-1}B +A'\Sigma^{-1}(\Sigma^{-1}+C'R^{-1}C+\Lambda^b_{t+1,t})^{-1}(\Sigma^{-1}B+C'R^{-1}y_t+\theta^b_{t+1,t})
\end{align*}
We see that these recursions agree with Fox's equation D.12, except she uses $\mu$ in place of $B$. She provides additional derivations to improve numerical stability (Algorithm 19). In particular, the messages are reparametrized using
\begin{align*}
\Lambda^b_{t|t} &= \Lambda^b_{t+1,t} + C'R^{-1}C\\
\theta^b_{t|t} &= \theta^b_{t+1,t} + C'R^{-1}y_t
\end{align*}
Once the backward messages have been computed, forward sampling of the hidden states $x_t$ is given from the recursion
\[p(x_t|x_{t-1},y_{1:T}) \propto p(x_t|x_{t-1})p(y_t|x_t)m_{t+1,t}(x_t)\]
After some algebra, this yields the following distribution for sampling:
\[p(x_t|x_{t-1},y_{1:T}) \propto \mathcal{N}^{-1}(x_t;~\Sigma^{-1}(Ax_{t-1}+B)+\theta^b_{t|t}~,~\Sigma^{-1}+\Lambda^b_{t|t})\]
(Fox's equation D.18). Note that generalization to a time-varying process with $A_t$,$B_t$, and $\Sigma_t$ known is straightforward.

\section{Unknown Dynamical Parameters}

Up to this point, we have assumed known dynamical parameters $A_t$, $\Sigma_t$, $B$, $C$, and $R$ and used these to sample from the state sequence $x_{1:T}$. We now reverse this procedure to sample from the dynamical parameters conditional on a state sequence. Throughout, we are conditioning on a fixed mode sequence $z_{1:T}$ which is sampled from the sticky HDP-HMM procedure. We continue to assume $C$ is fixed (for identifiability). Conditional on a known sequence of states computed by the Kalman smoother/sampler procedure, $A_t,\Sigma_t$ are independent of $R$. Focusing on $A_t$,$\Sigma_t$, Fox shows in thesis section 2.4.4 that the likelihood is that of a multivariate linear regression:
\[Y\sim\mathcal{MN}(AX+B,\Sigma,I)\]
The likelihood is that of a matrix normal distribution. If $Y\sim\mathcal{MN}(M,V,K)$ then $M$ is the mean parameter, $V$ is the row covariance parameter and $K^{-1}$ is the column covariance parameter\footnote{Fox's parameterization differs from common usage (cf wikipedia) in that the right covariance matrix here is $K^{-1}$ rather than K}. Our likelihood is a more general model than that presented by Fox, who sets $B=\mathbf{0}$. We modify the notation from that used in other sections for simplicity. In the context of the overall model, the $t^{th}$ column of $Y$ would correspond to $x_{t}$ and the $t^{th}$ column of $X$ would correspond to $x_{t-1}$. Also, as shown by Fox the model must also be split up based on the mode allocations $z_{1:T}$ from the HMM, but it turns out that each mode has its own conditionally independent multivariate linear regression, so we can ignore $z_{1:T}$ in the notation here.

The conjugate priors are:
\begin{align*}
\Sigma&\sim\mathcal{IW}(\nu,\Delta)\\
A|\Sigma&\sim\mathcal{MN}(M_A,\Sigma,K_A)\\
B|\Sigma&\sim\mathcal{MN}(M_B,\Sigma,K_B)
\end{align*}

While in the prior, $A$ and $B$ are specified as independent conditional on $\Sigma$, they become dependent in the posterior. Therefore, we modify Fox's result to specify full conditionals rather than a complete posterior. Let $D=\{X,Y\}$ represent the data. For the full conditional of $A$, note that conditional on $B$, we can replace $Y$ with $Y-B$ and obtain exactly the same distribution as derived by Fox for the special case of $B=0$. Therefore,
\[p(A|\Sigma,D,B)=\mathcal{MN}\left(A;~S_{ayx}S^{-1}_{axx},\Sigma,S_{axx}\right)\]
with slightly modified sufficient statistics
\begin{align*}
S_{axx} &= XX'+K_A\\
S_{ayx} &= (Y-B)X'+M_A K_A = YX'-BX'+M_A K_A\\
S_{ayy} &= (Y-B)(Y-B)'+M_A K_A M_A' = YY'-\left(BY'+(BY')'\right)+M_A K_A M_A'
\end{align*}
Similarly, conditional on $AX$, we can replace $Y$ with $Y-AX$ in the likelihood and consider the ``data'' for $B$ to be the identity matrix rather than the data matrix $X$. Again the distribution is just a reparameterized matrix normal likelihood, leading to the following full conditional:
\[p(B|\Sigma,D,A) = \mathcal{MN}\left(A;~S_{byx}S^{-1}_{bxx},\Sigma,S_{bxx}\right)\]
with modified sufficient statistics
\begin{align*}
S_{bxx} &= (II')+K_B = I+K_B\\
S_{byx} &= (Y-AX)I'+M_B K_B = Y-AX+M_B K_B\\
S_{byy} &= (Y-AX)(Y-AX)'+M_B K_B M_B' = YY' - \left(AXY'+(AXY')'\right)+M_B K_B M_B'
\end{align*}
Finally, to obtain the full conditional of $\Sigma$, note that we can again reparametrize to obtain a likelihood consistent with Fox's notation. Specifically, set
\begin{align*}
\tilde{X}&=\begin{pmatrix}X\\I\end{pmatrix}\\
\tilde{A}&=\begin{pmatrix}A & B\end{pmatrix}\\
\tilde{M}&=\begin{pmatrix}M_A & M_B\end{pmatrix}\\
\tilde{K}&=\begin{pmatrix}K_A & 0\\ 0 & K_B\end{pmatrix}
\end{align*}
Then the likelihood can be expressed as
\[Y\sim\mathcal{MN}(\tilde{A}\tilde{X},\Sigma,I)\]
and the (joint) prior for $\tilde{A}$ equivalent to
\[\tilde{A}|\Sigma\sim\mathcal{MN}(\tilde{M},\Sigma,\tilde{K})\]
At this point, we can just use the result from Fox's thesis (equation 2.99) to show that the full conditional of $\Sigma$ is given by
\[p(\Sigma|D)=\mathcal{IW}(\nu+N,~\Delta+S_{y|x})\]
where N is the number of columns in $Y$, $\nu$ and $\Delta$ are hyperparameters, and the sufficient statistic is
\[S_{y|x}=S_{yy}-S_{yx}S^{-1}_{xx}S_{yx}'\]
where
\begin{align*}
S_{yy} &= YY'+\tilde{M}\tilde{K}\tilde{M}' = YY'+(M_A,~M_B)\begin{pmatrix}K_A & 0 \\ 0 & K_b\end{pmatrix}\begin{pmatrix}M_A'\\M_B'\end{pmatrix}\\
&= YY'+M_A K_A M_A' + M_B K_B M_B'\\
S_{yx} &= Y\tilde{X}'+\tilde{M}\tilde{K} = Y(X',~I)+(M_A,~M_B)\begin{pmatrix}K_A & 0 \\ 0 & K_B\end{pmatrix}\\
&= (YX',~Y) + (M_A K_A,~ M_B K_B) = (YX'+M_AK_A,~Y+M_BK_B)\\
S_{xx} &= \tilde{X}\tilde{X}'+\tilde{K} = \begin{pmatrix}X\\I\end{pmatrix}(X',~I) + \begin{pmatrix}K_A & 0 \\ 0 & K_b\end{pmatrix}\\
&=\begin{pmatrix}K_A+XX' & X \\ X' & K_b+I\end{pmatrix} = \begin{pmatrix}S_{axx} & X \\ X' & S_{bxx}\end{pmatrix}
\end{align*}
Applying the standard block matrix inversion formula,
\begin{align*}
S_{xx}^{-1} &= \begin{pmatrix}\left(S_{axx}-X S_{bxx}^{-1}X'\right)^{-1} & -S_{axx}^{-1}X\left(S_{bxx}-X'S_{axx}^{-1}X\right)^{-1} \\ -S_{bxx}^{-1}X'\left(S_{axx}-XS_{bxx}^{-1}X'\right)^{-1} & \left(S_{bxx}-X'S_{axx}^{-1}X\right)^{-1}\end{pmatrix}\\
&= \begin{pmatrix}\Omega_a^{-1} & -S_{axx}^{-1}X\Omega_b^{-1}\\
                -S_{bxx}^{-1}X'\Omega_a^{-1} & \Omega_b^{-1}\end{pmatrix}
\end{align*}
Where $\Omega_a = S_{axx}-XS_{bxx}^{-1}X'$ and $\Omega_b = S_{bxx}-X'S_{axx}^{-1}X$. Note that due to symmetry, the off diagonal elements are just transposes of each other, so that $S_{axx}^{-1}X\Omega_{b}^{-1}=\left(S_{bxx}^{-1}X'\Omega_{a}^{-1}\right)' = \Omega_a^{-1}XS_{bxx}^{-1}$:
\[S_{xx}^{-1} = \begin{pmatrix}\Omega_a^{-1} & -\Omega_a^{-1}XS_{bxx}^{-1}\\
                -S_{bxx}^{-1}X'\Omega_a^{-1} & \Omega_b^{-1}\end{pmatrix}\]


Let $S_{yx}=(S_{yx1},S_{yx2})$. The quadratic form can then be expressed as
\begin{align*}
S_{yx}S_{xx}^{-1}S_{yx}' &= (S_{yx1},S_{yx2})\begin{pmatrix}\Omega_a^{-1} & -\Omega_a^{-1}XS_{bxx}^{-1}\\-S_{bxx}^{-1}X'\Omega_a^{-1} & \Omega_b^{-1}\end{pmatrix}\begin{pmatrix}S_{yx1}'\\S_{yx2}'\end{pmatrix}\\
&=S_{yx1}\Omega_a^{-1}S_{yx1}'+S_{yx2}\Omega_b^{-1}S_{yx2}'-S_{yx1}\Omega_a^{-1}XS_{bxx}^{-1}S_{yx2}'-\left(S_{yx1}\Omega_a^{-1}XS_{bxx}^{-1}S_{yx2}'\right)'
\end{align*}

Based on this result, we could also sample directly from the joint full conditional of $(A,B)=\tilde{A}$ rather than $A$ and $B$ separately by the following result:
\[p(\tilde{A}|\Sigma,D) = \mathcal{MN}\left(\tilde{A};~(S_{xx}^{-1}S_{yx}')',\Sigma,S_{xx}\right)\]
Where
\begin{align*}
S_{xx}^{-1}S_{yx}' &= \begin{pmatrix}\Omega_a^{-1} & -\Omega_a^{-1}XS_{bxx}^{-1}\\-S_{bxx}^{-1}X'\Omega_a^{-1} & \Omega_b^{-1}\end{pmatrix}\begin{pmatrix}S_{yx1}'\\S_{yx2}'\end{pmatrix}\\
&= \begin{pmatrix}\Omega_a^{-1}S_{yx1}' - \Omega_a^{-1}XS_{bxx}^{-1}S_{yx2}'\\\Omega_b^{-1}S_{yx1}' - S_{bxx}^{-1}X'\Omega_a^{-1}S_{yx1}'\end{pmatrix}
\end{align*}



\end{document}