---
title: "Multivariate and Matrix Normal Distributions"
author: "Will Townes"
date: "May 1, 2016"
output: html_document
---

```{r message=FALSE}
source("util.R") #where the workhorse functions are stored
```

### Cholesky Factors and Multivariate Normal Sampling

If we want to sample from $X\sim\mathcal{N}(\mu,\Sigma)$ it is easy using built-in functions. But if we have a Gaussian in information form, $X\sim\mathcal{N}^{-1}(\theta,\Lambda)$, we have to do expensive matrix inversions to reparameterize: $\Sigma=\Lambda^{-1}$ and $\mu=\Lambda^{-1}*\theta$. Instead, note that we can sample from $X$ by instead sampling $Z\sim\mathcal{N}(0,I)$ and transforming $X=U'Z+\mu$ where $\Sigma=U'U$. If $\Lambda=Q'Q$ then $\Sigma = \Lambda^{-1} = Q^{-1}\left(Q^{-1}\right)'$. Therefore $U' = Q^{-1}$ and we can express the transformation as $X=Q^{-1}Z+\Lambda^{-1}\theta$.

### Matrix Normal Distribution

Fox's definition of matrix normal differs from that used elsewhere. According to Fox, if $X\sim\mathcal{MN}(M,V,K)$ then $vec(X)\sim\mathcal{N}(vec(M),K^{-1}\otimes V)$. According to Wikipedia, this means that $V$ is the "among row" (or "left"?) covariance matrix while $K^{-1}$ is the "among column" (or "right"?) covariance matrix. Let $Q=K^{-1}$. Let $V=AA'$ and $Q=B'B$ (for example, by Cholesky decomposition). Sample a matrix of iid standard normal variates as $Z$. Then the following is a sample from the matrix-normal distribution with parameters $M,V,K$:
$$X=M+AZB$$
Suppose we are given $V,K$ as in the Fox parameterization. We can obtain $A$ and $B$ by the following procedure. In R $chol(Q)$ returns the matrix $B$ while $chol(V)$ returns $A'$. Note from previous discussion if $R=chol(K)$ then $(R')^{-1}=chol(K^{-1})=chol(Q)=B$.

### Testing the matrix normal sampler

Tests for the matrix normal sampling function. According to [wikipedia](https://en.wikipedia.org/wiki/Matrix_normal_distribution), if $X$ is distributed as matrix normal with mean $M$, row covariance $V$ and column covariance $Q$, then the following properties hold:
$$E[X] = M$$
$$E[(X-M)(X-M)'] = V* tr(Q)$$
$$E[(X-M)'(X-M)] = Q* tr(V)$$
We test that the empirical means of samples from our function agree with these properties.

```{r eval=FALSE}
m<-4
p<-5
M<-matrix(0,nrow=m,ncol=p)
V<-rbind(c(10,5,2,1),
         c(5.0,9,2,1),
         c(2,2,8,3),
         c(1,1,3,7))
Q<-rbind(c(10,5,0,0,0),
         c(5,10,5,0,0),
         c(0,5,10,5,0),
         c(0,0,5,10,5),
         c(0,0,0,5,10))
K<-solve(Q)
n<-100000 #larger values lead to lower MSE (higher accuracy)
res1<-rmatnorm(n,M,V,K,foxpar=TRUE)
res2<-rmatnorm(n,M,V,Q,foxpar=FALSE)
m<-nrow(M)
p<-ncol(M)
# Test of means equal to zero
mse(Reduce("+",res1)/n,M)
mse(Reduce("+",res2)/n,M)
# Test of row covariances
# Wikipedia: E[(X-M)(X-M)'] = V*trace(Q)
mse(V*trace(Q), list_mean(res1,function(x){(x-M)%*%t(x-M)}))
mse(V*trace(Q),list_mean(res2,function(x){(x-M)%*%t(x-M)}))
# Test of column covariances
# Wikipedia: E[(X-M)'(X-M)] = Q*trace(V)
mse(Q*trace(V),list_mean(res1,function(x){t(x-M)%*%(x-M)}))
mse(Q*trace(V),list_mean(res2,function(x){t(x-M)%*%(x-M)}))
```

### Bayesian Multivariate Linear Regression

Fox Thesis Section 2.4.4 shows that the matrix normal- inverse wishart prior is conjugate to the multivariate linear regression likelihood. The probability model is:
$$Y\sim\mathcal{MN}(AX+B,\Sigma,I)$$
$$\Sigma\sim\mathcal{IW}(\nu,\Delta)$$
$$A|\Sigma\sim\mathcal{MN}(M_A,\Sigma,K_A)$$
$$B|\Sigma\sim\mathcal{MN}(M_B,\Sigma,K_B)$$
For more detailed derivations, refer to the LDS block sampler latex/pdf files.

```{r}
#true values, similar to those from LDS.Rmd
k<-2 #seconds between observations
a<-c(0,-9.8) #acceleration
A<-diag(4)
A[1:2,3:4]<-k*diag(2)
B<-rbind(.5*k^2*diag(2),k*diag(2))%*%a
B<-B%*%rep(1,20) #copy vector many times to make a matrix
Sigma<-diag(c(20,20,10,10))
#generate fake data
X<-matrix(runif(80,-20,20),nrow=4)
Y<-rmatnorm(1,A%*%X+B,Sigma,diag(20))[[1]]

#default hyperparameter settings
d<-nrow(Y)
N<-ncol(Y)
nu0<-d+2
Delta0<-diag(d)
M_A<-matrix(0,nrow=d,ncol=d)
M_B<-matrix(0,nrow=d,ncol=N)
K_A<-diag(d)
K_B<-diag(N)
#Given hyperparameters and data, compute parameters of the posterior
#input: Y=outcome matrix
#X=input matrix
#model: Y=AX+B+E with E~MN(0,Sigma,I)
#nu0,Delta0=hyperparameters of Sigma
#M_A,K_A=hyperparameters of transition matrix A
#M_B,K_B=hyperparameters of additive matrix B
#output: list of parameters of the posterior
#in the output, A,B are concatenated column-wise into "AB"=[A,B]
#posterior params are for the concatenated matrix AB and Sigma
#nun,Deltan = params for posterior of Sigma
#M,K = params for posterior of AB given Sigma

```
