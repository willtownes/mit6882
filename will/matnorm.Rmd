---
title: "Multivariate and Matrix Normal Distributions"
author: "Will Townes"
date: "May 1, 2016"
output: html_document
---

```{r message=FALSE}
source("util.R") #where the workhorse functions are stored
```

### Cholesky Factors and Multivariate Normal Sampling

If we want to sample from $X\sim\mathcal{N}(\mu,\Sigma)$ it is easy using built-in functions. But if we have a Gaussian in information form, $X\sim\mathcal{N}^{-1}(\theta,\Lambda)$, we have to do expensive matrix inversions to reparameterize: $\Sigma=\Lambda^{-1}$ and $\mu=\Lambda^{-1}*\theta$. Instead, note that we can sample from $X$ by instead sampling $Z\sim\mathcal{N}(0,I)$ and transforming $X=U'Z+\mu$ where $\Sigma=U'U$. If $\Lambda=Q'Q$ then $\Sigma = \Lambda^{-1} = Q^{-1}\left(Q^{-1}\right)'$. Therefore $U' = Q^{-1}$ and we can express the transformation as $X=Q^{-1}Z+\Lambda^{-1}\theta$.

### Matrix Normal Distribution

Fox's definition of matrix normal differs from that used elsewhere. According to Fox, if $X\sim\mathcal{MN}(M,V,K)$ then $vec(X)\sim\mathcal{N}(vec(M),K^{-1}\otimes V)$. According to Wikipedia, this means that $V$ is the "among row" (or "left"?) covariance matrix while $K^{-1}$ is the "among column" (or "right"?) covariance matrix. Let $Q=K^{-1}$. Let $V=AA'$ and $Q=B'B$ (for example, by Cholesky decomposition). Sample a matrix of iid standard normal variates as $Z$. Then the following is a sample from the matrix-normal distribution with parameters $M,V,K$:
$$X=M+AZB$$
Suppose we are given $V,K$ as in the Fox parameterization. We can obtain $A$ and $B$ by the following procedure. In R $chol(Q)$ returns the matrix $B$ while $chol(V)$ returns $A'$. Note from previous discussion if $R=chol(K)$ then $(R')^{-1}=chol(K^{-1})=chol(Q)=B$.

### Testing the matrix normal sampler

Tests for the matrix normal sampling function. According to [wikipedia](https://en.wikipedia.org/wiki/Matrix_normal_distribution), if $X$ is distributed as matrix normal with mean $M$, row covariance $V$ and column covariance $Q$, then the following properties hold:
$$E[X] = M$$
$$E[(X-M)(X-M)'] = V* tr(Q)$$
$$E[(X-M)'(X-M)] = Q* tr(V)$$
We test that the empirical means of samples from our function agree with these properties.

```{r eval=TRUE}
m<-4
p<-5
M<-matrix(0,nrow=m,ncol=p)
V<-rbind(c(10,5,2,1),
         c(5.0,9,2,1),
         c(2,2,8,3),
         c(1,1,3,7))
Q<-rbind(c(10,5,0,0,0),
         c(5,10,5,0,0),
         c(0,5,10,5,0),
         c(0,0,5,10,5),
         c(0,0,0,5,10))
K<-solve(Q)
n<-100000 #larger values lead to lower MSE (higher accuracy)
res1<-rmatnorm(n,M,V,K,foxpar=TRUE)
res2<-rmatnorm(n,M,V,Q,foxpar=FALSE)
m<-nrow(M)
p<-ncol(M)
# Test of means equal to zero
mse(Reduce("+",res1)/n,M)
mse(Reduce("+",res2)/n,M)
# Test of row covariances
# Wikipedia: E[(X-M)(X-M)'] = V*trace(Q)
mse(V*trace(Q), list_mean(res1,function(x){(x-M)%*%t(x-M)}))
mse(V*trace(Q),list_mean(res2,function(x){(x-M)%*%t(x-M)}))
# Test of column covariances
# Wikipedia: E[(X-M)'(X-M)] = Q*trace(V)
mse(Q*trace(V),list_mean(res1,function(x){t(x-M)%*%(x-M)}))
mse(Q*trace(V),list_mean(res2,function(x){t(x-M)%*%(x-M)}))
```

